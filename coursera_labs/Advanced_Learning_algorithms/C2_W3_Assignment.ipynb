{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Lab: Advice for Applying Machine Learning\n",
    "In this lab, you will explore techniques to evaluate and improve your machine learning models.\n",
    "\n",
    "# Outline\n",
    "- [ 1 - Packages ](#1)\n",
    "- [ 2 - Evaluating a Learning Algorithm (Polynomial Regression)](#2)\n",
    "  - [ 2.1 Splitting your data set](#2.1)\n",
    "  - [ 2.2 Error calculation for model evaluation, linear regression](#2.2)\n",
    "    - [ Exercise 1](#ex01)\n",
    "  - [ 2.3 Compare performance on training and test data](#2.3)\n",
    "- [ 3 - Bias and Variance<img align=\"Right\" src=\"./images/C2_W3_BiasVarianceDegree.png\"  style=\" width:500px; padding: 10px 20px ; \"> ](#3)\n",
    "  - [ 3.1 Plot Train, Cross-Validation, Test](#3.1)\n",
    "  - [ 3.2 Finding the optimal degree](#3.2)\n",
    "  - [ 3.3 Tuning Regularization.](#3.3)\n",
    "  - [ 3.4 Getting more data: Increasing Training Set Size (m)](#3.4)\n",
    "- [ 4 - Evaluating a Learning Algorithm (Neural Network)](#4)\n",
    "  - [ 4.1 Data Set](#4.1)\n",
    "  - [ 4.2 Evaluating categorical model by calculating classification error](#4.2)\n",
    "    - [ Exercise 2](#ex02)\n",
    "- [ 5 - Model Complexity](#5)\n",
    "  - [ Exercise 3](#ex03)\n",
    "  - [ 5.1 Simple model](#5.1)\n",
    "    - [ Exercise 4](#ex04)\n",
    "- [ 6 - Regularization](#6)\n",
    "  - [ Exercise 5](#ex05)\n",
    "- [ 7 - Iterate to find optimal regularization value](#7)\n",
    "  - [ 7.1 Test](#7.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - Packages \n",
    "\n",
    "First, let's run the cell below to import all the packages that you will need during this assignment.\n",
    "- [numpy](https://numpy.org/) is the fundamental package for scientific computing Python.\n",
    "- [matplotlib](http://matplotlib.org) is a popular library to plot graphs in Python.\n",
    "- [scikitlearn](https://scikit-learn.org/stable/) is a basic library for data mining\n",
    "- [tensorflow](https://www.tensorflow.org/) a popular platform for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.activation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m relu, linear\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlosses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseCategoricalCrossentropy\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.activation'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.activation import relu, linear\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "from public_tests_a1 import *\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "from assignment_utils import *\n",
    "\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2 - Evaluating a Learning Algorithm (Polynomial Regression)\n",
    "\n",
    "<img align=\"Right\" src=\"./images/C2_W3_TrainingVsNew.png\"  style=\" width:350px; padding: 10px 20px ; \"> Let's say you have created a machine learning model and you find it *fits* your training data very well. You're done? Not quite. The goal of creating the model was to be able to predict values for <span style=\"color:blue\">*new* </span> examples. \n",
    "\n",
    "How can you test your model's performance on new data before deploying it?   \n",
    "The answer has two parts:\n",
    "* Split your original data set into \"Training\" and \"Test\" sets. \n",
    "    * Use the training data to fit the parameters of the model\n",
    "    * Use the test data to evaluate the model on *new* data\n",
    "* Develop an error function to evaluate your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.1\"></a>\n",
    "### 2.1 Splitting your data set\n",
    "Lectures advised reserving 20-40% of your data set for testing. Let's use an `sklearn` function [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to perform the split. Double-check the shapes after running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gen_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate some data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X,y,x_ideal,y_ideal \u001b[38;5;241m=\u001b[39m gen_data(\u001b[38;5;241m18\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0.7\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX.shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my.shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Split the data using sklearn routine\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gen_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate some data\n",
    "X,y,x_ideal,y_ideal = gen_data(18, 2, 0.7)\n",
    "print(\"X.shape\", X.shape, \"y.shape\", y.shape)\n",
    "\n",
    "# Split the data using sklearn routine\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "print(\"X_train.shape\", X_train.shape, \"y_train.shape\", y_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape, \"y_test.shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Plot Train, Test sets\n",
    "You can see below the data points that will be part of training (in red) are intermixed with those that the model is not trained on (test). This particular data set is a quadratic function with noise added. The \"ideal\" curve is shown for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.2\"></a>\n",
    "### 2.2 Error calculation for model evaluation, linear regression\n",
    "When *evaluating* a linear regression model, you average the squared error difference of the predicted values and the target values.\n",
    "\n",
    "$$ J_\\text{test}(\\mathbf{w},b) = \n",
    "            \\frac{1}{2m_\\text{test}}\\sum_{i=0}^{m_\\text{test}-1} ( f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}_\\text{test}) - y^{(i)}_\\text{test} )^2 \n",
    "            \\tag{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex01\"></a>\n",
    "### Exercise 1\n",
    "\n",
    "Below, create a function to evaluate the error on a data set for a linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1\n",
    "# GRADED CELL: eval_mse\n",
    "def eval_mse(y, yhat):\n",
    "    \"\"\" \n",
    "    Calculate the mean squared error on a data set.\n",
    "    Args:\n",
    "      y    : (ndarray  Shape (m,) or (m,1))  target value of each example\n",
    "      yhat : (ndarray  Shape (m,) or (m,1))  predicted value of each example\n",
    "    Returns:\n",
    "      err: (scalar)             \n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    err = 0.0\n",
    "    for i in range(m):\n",
    "    ### START CODE HERE ### \n",
    "        m = len(y)\n",
    "        err = 0.0\n",
    "        for i in range(m):\n",
    "            err_i  = ( (yhat[i] - y[i])**2 ) \n",
    "            err   += err_i                                                                \n",
    "        err = err / (2*m)    \n",
    "        \n",
    "    \n",
    "    ### END CODE HERE ### \n",
    "    \n",
    "    return(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.3\"></a>\n",
    "### 2.3 Compare performance on training and test data\n",
    "Let's build a high degree polynomial model to minimize training error. This will use the linear_regression functions from `sklearn`. The code is in the imported utility file if you would like to see the details. The steps below are:\n",
    "* create and fit the model. ('fit' is another name for training or running gradient descent).\n",
    "* compute the error on the training data.\n",
    "* compute the error on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lin_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# create a model in sklearn, train on training data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m degree \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 3\u001b[0m lmodel \u001b[38;5;241m=\u001b[39m lin_model(degree)\n\u001b[1;32m      4\u001b[0m lmodel\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# predict on training data, find training error\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lin_model' is not defined"
     ]
    }
   ],
   "source": [
    "# create a model in sklearn, train on training data\n",
    "degree = 10\n",
    "lmodel = lin_model(degree)\n",
    "lmodel.fit(X_train, y_train)\n",
    "\n",
    "# predict on training data, find training error\n",
    "yhat = lmodel.predict(X_train)\n",
    "err_train = lmodel.mse(y_train, yhat)\n",
    "\n",
    "# predict on test data, find error\n",
    "yhat = lmodel.predict(X_test)\n",
    "err_test = lmodel.mse(y_test, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set error shows this model will not work well on new data. If you use the test error to guide improvements in the model, then the model will perform well on the test data... but the test data was meant to represent *new* data.\n",
    "You need yet another set of data to test new data performance.\n",
    "\n",
    "The proposal made during lecture is to separate data into three groups. The distribution of training, cross-validation and test sets shown in the below table is a typical distribution, but can be varied depending on the amount of data available.\n",
    "\n",
    "| data             | % of total | Description |\n",
    "|------------------|:----------:|:---------|\n",
    "| training         | 60         | Data used to tune model parameters $w$ and $b$ in training or fitting |\n",
    "| cross-validation | 20         | Data used to tune other model parameters like degree of polynomial, regularization or the architecture of a neural network.|\n",
    "| test             | 20         | Data used to test the model after tuning to gauge performance on new data |\n",
    "\n",
    "\n",
    "Let's generate three data sets below. We'll once again use `train_test_split` from `sklearn` but will call it twice to get three splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gen_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate  data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X,y, x_ideal,y_ideal \u001b[38;5;241m=\u001b[39m gen_data(\u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m0.7\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX.shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my.shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#split the data using sklearn routine \u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gen_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate  data\n",
    "X,y, x_ideal,y_ideal = gen_data(40, 5, 0.7)\n",
    "print(\"X.shape\", X.shape, \"y.shape\", y.shape)\n",
    "\n",
    "#split the data using sklearn routine \n",
    "X_train, X_, y_train, y_ = train_test_split(X,y,test_size=0.40, random_state=1)\n",
    "X_cv, X_test, y_cv, y_test = train_test_split(X_,y_,test_size=0.50, random_state=1)\n",
    "print(\"X_train.shape\", X_train.shape, \"y_train.shape\", y_train.shape)\n",
    "print(\"X_cv.shape\", X_cv.shape, \"y_cv.shape\", y_cv.shape)\n",
    "print(\"X_test.shape\", X_test.shape, \"y_test.shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## 3 - Bias and Variance<img align=\"Right\" src=\"./images/C2_W3_BiasVarianceDegree.png\"  style=\" width:500px; padding: 10px 20px ; \"> \n",
    " Above, it was clear the degree of the polynomial model was too high. How can you choose a good value? It turns out, as shown in the diagram, the training and cross-validation performance can provide guidance. By trying a range of degree values, the training and cross-validation performance can be evaluated. As the degree becomes too large, the cross-validation performance will start to degrade relative to the training performance. Let's try this on our example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.1\"></a>\n",
    "### 3.1 Plot Train, Cross-Validation, Test\n",
    "You can see below the datapoints that will be part of training (in red) are intermixed with those that the model is not trained on (test and cv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_ideal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m ax\u001b[38;5;241m.\u001b[39mplot(x_ideal, y_ideal, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m, color \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morangered\u001b[39m\u001b[38;5;124m\"\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_ideal\u001b[39m\u001b[38;5;124m\"\u001b[39m, lw\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining, CV, Test\u001b[39m\u001b[38;5;124m\"\u001b[39m,fontsize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m14\u001b[39m)\n\u001b[1;32m      4\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_xlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_ideal' is not defined"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d648d77da7ca498d88bf0c58e3689d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAGQCAYAAACAvzbMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAaO0lEQVR4nO3de2zV9f3H8ddpDz0Ft3OMoLVArcXBrJLhaANS1hid1gDBYVyocaHAMLFRx6XDSe0iQkwaXSQTpfVCCzEB14Bi+KNTT+IG5bILXWuMbaIRZkFbSWs8p94OUj6/Pwgnv2OLct58T3s6n4/k/HE++35P3x9x5+n3nJ6DzznnBABAkjJGegAAwOhEQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGDieUD279+vhQsXauLEifL5fHrttde+95x9+/apqKhI2dnZmjJlip577jmvxwIAeMzzgHzxxReaMWOGnn322Qs6/tixY5o/f75KS0vV1tamRx55RCtXrtQrr7zi9WgAAA/5nHMuZQ/u82nPnj1atGjReY95+OGHtXfvXnV2dsbXKisr9fbbb+vw4cOpGg0AcJH8Iz3A4cOHVVZWlrB2++23q6GhQd98843GjBkz6JxYLKZYLBa/f+bMGX366acaP368fD5fymcGgFRzzqm/v18TJ05URkZ6vl094gHp6elRTk5OwlpOTo5Onz6t3t5e5ebmDjqntrZWGzZsGK4RAWDEHD9+XJMnTx7pMYY04gGRNOiq4dyraue7mqiurlZVVVX8fiQS0VVXXaXjx48rGAymblAAGCbRaFR5eXn68Y9/PNKjnNeIB+TKK69UT09PwtrJkyfl9/s1fvz4Ic8JBAIKBAKD1oPBIAEB8D8lnV+WH/EX1ubMmaNwOJyw9uabb6q4uHjI9z8AAOnB84B8/vnnam9vV3t7u6Szv6bb3t6urq4uSWdffqqoqIgfX1lZqQ8//FBVVVXq7OxUY2OjGhoatHbtWq9HAwB4yPOXsI4cOaKbb745fv/cexVLly7V9u3b1d3dHY+JJBUUFKi5uVlr1qzRli1bNHHiRG3evFl33XWX16MBADyU0s+BDJdoNKpQKKRIJMJ7IAD+J4yG57URfw8EADA6ERAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgElKAlJXV6eCggJlZ2erqKhILS0t33n8jh07NGPGDI0bN065ublavny5+vr6UjEaAMAjngekqalJq1evVk1Njdra2lRaWqp58+apq6tryOMPHDigiooKrVixQu+++6527dqlf//737r33nu9Hg0A4CHPA7Jp0yatWLFC9957rwoLC/XnP/9ZeXl5qq+vH/L4f/zjH7r66qu1cuVKFRQU6Be/+IXuu+8+HTlyxOvRAAAe8jQgp06dUmtrq8rKyhLWy8rKdOjQoSHPKSkp0YkTJ9Tc3CznnD755BPt3r1bCxYs8HI0AIDHPA1Ib2+vBgYGlJOTk7Cek5Ojnp6eIc8pKSnRjh07VF5erqysLF155ZW69NJL9cwzz5z358RiMUWj0YQbAGB4peRNdJ/Pl3DfOTdo7ZyOjg6tXLlSjz76qFpbW/X666/r2LFjqqysPO/j19bWKhQKxW95eXmezg8A+H4+55zz6sFOnTqlcePGadeuXbrzzjvj66tWrVJ7e7v27ds36JwlS5bo66+/1q5du+JrBw4cUGlpqT7++GPl5uYOOicWiykWi8XvR6NR5eXlKRKJKBgMerUdABgx0WhUoVAorZ/XPL0CycrKUlFRkcLhcMJ6OBxWSUnJkOd8+eWXyshIHCMzM1PS2SuXoQQCAQWDwYQbAGB4ef4SVlVVlbZu3arGxkZ1dnZqzZo16urqir8kVV1drYqKivjxCxcu1Kuvvqr6+nodPXpUBw8e1MqVKzVr1ixNnDjR6/EAAB7xe/2A5eXl6uvr08aNG9Xd3a3p06erublZ+fn5kqTu7u6Ez4QsW7ZM/f39evbZZ/X73/9el156qW655RY98cQTXo8GAPCQp++BjJTR8FohACRjNDyv8V1YAAATAgIAMCEgAAATAgIAMCEgAAATAgIAMCEgAAATAgIAMCEgAAATAgIAMCEgAAATAgIAMCEgAAATAgIAMCEgAAATAgIAMCEgAAATAgIAMCEgAAATAgIAMCEgAAATAgIAMCEgAAATAgIAMCEgAAATAgIAMCEgAAATAgIAMCEgAAATAgIAMCEgAAATAgIAMCEgAAATAgIAMCEgAAATAgIAMCEgAAATAgIAMCEgAAATAgIAMCEgAAATAgIAMCEgAAATAgIAMCEgAAATAgIAMCEgAAATAgIAMCEgAAATAgIAMCEgAACTlASkrq5OBQUFys7OVlFRkVpaWr7z+FgsppqaGuXn5ysQCOiaa65RY2NjKkYDAHjE7/UDNjU1afXq1aqrq9PcuXP1/PPPa968eero6NBVV1015DmLFy/WJ598ooaGBv3kJz/RyZMndfr0aa9HAwB4yOecc14+4OzZszVz5kzV19fH1woLC7Vo0SLV1tYOOv7111/X3XffraNHj+qyyy4z/cxoNKpQKKRIJKJgMGieHQDSxWh4XvP0JaxTp06ptbVVZWVlCetlZWU6dOjQkOfs3btXxcXFevLJJzVp0iRNmzZNa9eu1VdffXXenxOLxRSNRhNuAIDh5elLWL29vRoYGFBOTk7Cek5Ojnp6eoY85+jRozpw4ICys7O1Z88e9fb26v7779enn3563vdBamtrtWHDBi9HBwAkKSVvovt8voT7zrlBa+ecOXNGPp9PO3bs0KxZszR//nxt2rRJ27dvP+9VSHV1tSKRSPx2/Phxz/cAAPhunl6BTJgwQZmZmYOuNk6ePDnoquSc3NxcTZo0SaFQKL5WWFgo55xOnDihqVOnDjonEAgoEAh4OToAIEmeXoFkZWWpqKhI4XA4YT0cDqukpGTIc+bOnauPP/5Yn3/+eXztvffeU0ZGhiZPnuzleAAAD3n+ElZVVZW2bt2qxsZGdXZ2as2aNerq6lJlZaWksy8/VVRUxI+/5557NH78eC1fvlwdHR3av3+/HnroIf32t7/V2LFjvR4PAOARzz8HUl5err6+Pm3cuFHd3d2aPn26mpublZ+fL0nq7u5WV1dX/Pgf/ehHCofD+t3vfqfi4mKNHz9eixcv1uOPP+71aAAAD3n+OZCRMBp+XxoAkjEantf4LiwAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYEBAAgAkBAQCYpCQgdXV1KigoUHZ2toqKitTS0nJB5x08eFB+v1833HBDKsYCAHjI84A0NTVp9erVqqmpUVtbm0pLSzVv3jx1dXV953mRSEQVFRX65S9/6fVIAIAU8DnnnJcPOHv2bM2cOVP19fXxtcLCQi1atEi1tbXnPe/uu+/W1KlTlZmZqddee03t7e0X/DOj0ahCoZAikYiCweDFjA8AaWE0PK95egVy6tQptba2qqysLGG9rKxMhw4dOu9527Zt0wcffKD169d7OQ4AIIX8Xj5Yb2+vBgYGlJOTk7Cek5Ojnp6eIc95//33tW7dOrW0tMjvv7BxYrGYYrFY/H40GrUPDQAwScmb6D6fL+G+c27QmiQNDAzonnvu0YYNGzRt2rQLfvza2lqFQqH4LS8v76JnBgAkx9OATJgwQZmZmYOuNk6ePDnoqkSS+vv7deTIET344IPy+/3y+/3auHGj3n77bfn9fr311ltD/pzq6mpFIpH47fjx415uAwBwATx9CSsrK0tFRUUKh8O688474+vhcFi/+tWvBh0fDAb1zjvvJKzV1dXprbfe0u7du1VQUDDkzwkEAgoEAl6ODgBIkqcBkaSqqiotWbJExcXFmjNnjl544QV1dXWpsrJS0tmrh48++kgvvfSSMjIyNH369ITzr7jiCmVnZw9aBwCkF88DUl5err6+Pm3cuFHd3d2aPn26mpublZ+fL0nq7u7+3s+EAADSn+efAxkJo+H3pQEgGaPheY3vwgIAmBAQAIAJAQEAmBAQAIAJAQEAmBAQAIAJAQEAmBAQAIAJAQEAmBAQAIAJAQEAmBAQAIAJAQEAmBAQAIAJAQEAmBAQAIAJAQEAmBAQAIAJAQEAmBAQAIAJAQEAmBAQAIAJAQEAmBAQAIAJAQEAmBAQAIAJAQEAmBAQAIAJAQEAmBAQAIAJAQEAmBAQAIAJAQEAmBAQAIAJAQEAmBAQAIAJAQEAmBAQAIAJAQEAmBAQAIAJAQEAmBAQAIAJAQEAmBAQAIAJAQEAmBAQAIAJAQEAmBAQAIAJAQEAmBAQAIAJAQEAmKQkIHV1dSooKFB2draKiorU0tJy3mNfffVV3Xbbbbr88ssVDAY1Z84cvfHGG6kYCwDgIc8D0tTUpNWrV6umpkZtbW0qLS3VvHnz1NXVNeTx+/fv12233abm5ma1trbq5ptv1sKFC9XW1ub1aAAAD/mcc87LB5w9e7Zmzpyp+vr6+FphYaEWLVqk2traC3qM66+/XuXl5Xr00Ucv6PhoNKpQKKRIJKJgMGiaGwDSyWh4XvP0CuTUqVNqbW1VWVlZwnpZWZkOHTp0QY9x5swZ9ff367LLLjvvMbFYTNFoNOEGABhengakt7dXAwMDysnJSVjPyclRT0/PBT3GU089pS+++EKLFy8+7zG1tbUKhULxW15e3kXNDQBIXkreRPf5fAn3nXOD1oby8ssv67HHHlNTU5OuuOKK8x5XXV2tSCQSvx0/fvyiZwYAJMfv5YNNmDBBmZmZg642Tp48Oeiq5Nuampq0YsUK7dq1S7feeut3HhsIBBQIBC56XgCAnadXIFlZWSoqKlI4HE5YD4fDKikpOe95L7/8spYtW6adO3dqwYIFXo4EAEgRT69AJKmqqkpLlixRcXGx5syZoxdeeEFdXV2qrKyUdPblp48++kgvvfSSpLPxqKio0NNPP60bb7wxfvUyduxYhUIhr8cDAHjE84CUl5err69PGzduVHd3t6ZPn67m5mbl5+dLkrq7uxM+E/L888/r9OnTeuCBB/TAAw/E15cuXart27d7PR4AwCOefw5kJIyG35cGgGSMhuc1vgsLAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGBCQAAAJgQEAGCSkoDU1dWpoKBA2dnZKioqUktLy3cev2/fPhUVFSk7O1tTpkzRc889l4qxAAAe8jwgTU1NWr16tWpqatTW1qbS0lLNmzdPXV1dQx5/7NgxzZ8/X6WlpWpra9MjjzyilStX6pVXXvF6NACAh3zOOeflA86ePVszZ85UfX19fK2wsFCLFi1SbW3toOMffvhh7d27V52dnfG1yspKvf322zp8+PAF/cxoNKpQKKRIJKJgMHjxmwCAETYantf8Xj7YqVOn1NraqnXr1iWsl5WV6dChQ0Oec/jwYZWVlSWs3X777WpoaNA333yjMWPGDDonFospFovF70ciEUln/4EDwP+Cc89nHv83vqc8DUhvb68GBgaUk5OTsJ6Tk6Oenp4hz+np6Rny+NOnT6u3t1e5ubmDzqmtrdWGDRsGrefl5V3E9ACQfvr6+hQKhUZ6jCF5GpBzfD5fwn3n3KC17zt+qPVzqqurVVVVFb//2WefKT8/X11dXWn7DzoVotGo8vLydPz48bS9xE0F9s2+fwgikYiuuuoqXXbZZSM9ynl5GpAJEyYoMzNz0NXGyZMnB11lnHPllVcOebzf79f48eOHPCcQCCgQCAxaD4VCP6h/wc4JBoPs+weEff+wZGSk76ctPJ0sKytLRUVFCofDCevhcFglJSVDnjNnzpxBx7/55psqLi4e8v0PAEB68DxtVVVV2rp1qxobG9XZ2ak1a9aoq6tLlZWVks6+/FRRURE/vrKyUh9++KGqqqrU2dmpxsZGNTQ0aO3atV6PBgDwkOfvgZSXl6uvr08bN25Ud3e3pk+frubmZuXn50uSuru7Ez4TUlBQoObmZq1Zs0ZbtmzRxIkTtXnzZt11110X/DMDgYDWr18/5Mta/8vYN/v+IWDf6btvzz8HAgD4YUjfd2cAAGmNgAAATAgIAMCEgAAATEZNQH6oXxGfzL5fffVV3Xbbbbr88ssVDAY1Z84cvfHGG8M4rXeS/fM+5+DBg/L7/brhhhtSO2CKJLvvWCymmpoa5efnKxAI6JprrlFjY+MwTeudZPe9Y8cOzZgxQ+PGjVNubq6WL1+uvr6+YZr24u3fv18LFy7UxIkT5fP59Nprr33vOWn5nOZGgb/85S9uzJgx7sUXX3QdHR1u1apV7pJLLnEffvjhkMcfPXrUjRs3zq1atcp1dHS4F1980Y0ZM8bt3r17mCe/OMnue9WqVe6JJ55w//rXv9x7773nqqur3ZgxY9x//vOfYZ784iS773M+++wzN2XKFFdWVuZmzJgxPMN6yLLvO+64w82ePduFw2F37Ngx989//tMdPHhwGKe+eMnuu6WlxWVkZLinn37aHT161LW0tLjrr7/eLVq0aJgnt2tubnY1NTXulVdecZLcnj17vvP4dH1OGxUBmTVrlqusrExYu/baa926deuGPP4Pf/iDu/baaxPW7rvvPnfjjTembMZUSHbfQ7nuuuvchg0bvB4tpaz7Li8vd3/84x/d+vXrR2VAkt33X//6VxcKhVxfX99wjJcyye77T3/6k5syZUrC2ubNm93kyZNTNmMqXUhA0vU5Le1fwjr3FfHf/sp3y1fEHzlyRN98803KZvWSZd/fdubMGfX396f1l7F9m3Xf27Zt0wcffKD169enesSUsOx77969Ki4u1pNPPqlJkyZp2rRpWrt2rb766qvhGNkTln2XlJToxIkTam5ulnNOn3zyiXbv3q0FCxYMx8gjIl2f01LybbxeGq6viE83ln1/21NPPaUvvvhCixcvTsWIKWHZ9/vvv69169appaVFfn/a/ys9JMu+jx49qgMHDig7O1t79uxRb2+v7r//fn366aej5n0Qy75LSkq0Y8cOlZeX6+uvv9bp06d1xx136JlnnhmOkUdEuj6npf0VyDmp/or4dJXsvs95+eWX9dhjj6mpqUlXXHFFqsZLmQvd98DAgO655x5t2LBB06ZNG67xUiaZP+8zZ87I5/Npx44dmjVrlubPn69NmzZp+/bto+oqREpu3x0dHVq5cqUeffRRtba26vXXX9exY8fi37f3vyodn9PS/j/Xhusr4tONZd/nNDU1acWKFdq1a5duvfXWVI7puWT33d/fryNHjqitrU0PPvigpLNPrM45+f1+vfnmm7rllluGZfaLYfnzzs3N1aRJkxL+DpzCwkI553TixAlNnTo1pTN7wbLv2tpazZ07Vw899JAk6Wc/+5kuueQSlZaW6vHHHx8VrzAkK12f09L+CuSH+hXxln1LZ688li1bpp07d47K14ST3XcwGNQ777yj9vb2+K2yslI//elP1d7ertmzZw/X6BfF8uc9d+5cffzxx/r888/ja++9954yMjI0efLklM7rFcu+v/zyy0F/R0ZmZqak9P7rXy9G2j6njdCb90k592t+DQ0NrqOjw61evdpdcskl7r///a9zzrl169a5JUuWxI8/9ytva9ascR0dHa6hoSEtfuUtWcnue+fOnc7v97stW7a47u7u+O2zzz4bqS2YJLvvbxutv4WV7L77+/vd5MmT3a9//Wv37rvvun379rmpU6e6e++9d6S2YJLsvrdt2+b8fr+rq6tzH3zwgTtw4IArLi52s2bNGqktJK2/v9+1tbW5trY2J8lt2rTJtbW1xX91ebQ8p42KgDjn3JYtW1x+fr7LyspyM2fOdPv27Yv/b0uXLnU33XRTwvF///vf3c9//nOXlZXlrr76aldfXz/ME3sjmX3fdNNNTtKg29KlS4d/8IuU7J/3/zdaA+Jc8vvu7Ox0t956qxs7dqybPHmyq6qqcl9++eUwT33xkt335s2b3XXXXefGjh3rcnNz3W9+8xt34sSJYZ7a7m9/+9t3/n91tDyn8XXuAACTtH8PBACQnggIAMCEgAAATAgIAMCEgAAATAgIAMCEgAAATAgIAMCEgAAATAgIAMCEgAAATAgIAMCEgAAATAgIAMCEgAAATAgIAMCEgAAATAgIAMCEgAAATAgIAMCEgAAATAgIAMDk/wAtWD5jCTEbtQAAAABJRU5ErkJggg==",
      "text/html": [
       "\n",
       "            <div style=\"display: inline-block;\">\n",
       "                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n",
       "                    Figure\n",
       "                </div>\n",
       "                <img src='data:image/png;base64,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' width=400.0/>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(4,4))\n",
    "ax.plot(x_ideal, y_ideal, \"--\", color = \"orangered\", label=\"y_ideal\", lw=1)\n",
    "ax.set_title(\"Training, CV, Test\",fontsize = 14)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "\n",
    "ax.scatter(X_train, y_train, color = \"red\",           label=\"train\")\n",
    "ax.scatter(X_cv, y_cv,       color = dlc[\"dlorange\"], label=\"cv\")\n",
    "ax.scatter(X_test, y_test,   color = dlc[\"dlblue\"],   label=\"test\")\n",
    "ax.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.2\"></a>\n",
    "### 3.2 Finding the optimal degree\n",
    "In previous labs, you found that you could create a model capable of fitting complex curves by utilizing a polynomial (See Course1, Week2 Feature Engineering and Polynomial Regression Lab).  Further, you demonstrated that by increasing the *degree* of the polynomial, you could *create* overfitting. (See Course 1, Week3, Over-Fitting Lab). Let's use that knowledge here to test our ability to tell the difference between over-fitting and under-fitting.\n",
    "\n",
    "Let's train the model repeatedly, increasing the degree of the polynomial each iteration. Here, we're going to use the [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) linear regression model for speed and simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://localhost:8888/'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "max_degree = 9\n",
    "err_train = np.zeros(max_degree)\n",
    "err_cv = np.zeros(max_degree)\n",
    "x = np.linspace(0, int(X.max()), 100)\n",
    "y_pred = np.zeros((100,max_degree))\n",
    "\n",
    "for degree in range(max_degree):\n",
    "    lmodel = lin_model(degree+1)\n",
    "    lmodel.fit(X_train, y_train)\n",
    "    yhat = lmodel.predict(X_train)\n",
    "    err_train[degree] = lmodel.mse(y_train, yhat)\n",
    "    yhat = lmodel.predict(X_cv)\n",
    "    err_cv[degree] = lmodel.mse(y_cv, yhat)\n",
    "    y_pred[:,degree] = lmodel.predict(x)\n",
    "\n",
    "optimal_degree = np.argmin(err_cv)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.3\"></a>\n",
    "### 3.3 Tuning Regularization.\n",
    "In previous labs, you have utilized *regularization* to reduce overfitting. Similar to degree, one can use the same methodology to tune the regularization parameter lambda ($\\lambda$).\n",
    "\n",
    "Let's demonstrate this by starting with a high degree polynomial and varying the regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_range = np.array([0.0. 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0, 10.0, 100.0])\n",
    "num_steps = len(lambda_range)\n",
    "degree = 10\n",
    "err_train = np.zeros(num_steps)    \n",
    "err_cv = np.zeros(num_steps)       \n",
    "x = np.linspace(0,int(X.max()),100) \n",
    "y_pred = np.zeros((100,num_steps))  #columns are lines to plot\n",
    "\n",
    "for i in range(num_steps):\n",
    "    lambda_= lambda_range[i]\n",
    "    lmodel = lin_model(degree, regularization=True, lambda_=lambda_)\n",
    "    lmodel.fit(X_train, y_train)\n",
    "    yhat = lmodel.predict(X_train)\n",
    "    err_train[i] = lmodel.mse(y_train, yhat)\n",
    "    yhat = lmodel.predict(X_cv)\n",
    "    err_cv[i] = lmodel.mse(y_cv, yhat)\n",
    "    y_pred[:,i] = lmodel.predict(x)\n",
    "    \n",
    "optimal_reg_idx = np.argmin(err_cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.4\"></a>\n",
    "### 3.4 Getting more data: Increasing Training Set Size (m)\n",
    "When a model is overfitting (high variance), collecting additional data can improve performance. Let's try that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_cv, y_cv, x, y_pred, err_train, err_cv, m_range,degree = tune_m()\n",
    "plt_tune_m(X_train, y_train, X_cv, y_cv, x, y_pred, err_train, err_cv, m_range, degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## 4 - Evaluating a Learning Algorithm (Neural Network)\n",
    "Above, you tuned aspects of a polynomial regression model. Here, you will work with a neural network model. Let's start by creating a classification data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4.1\"></a>\n",
    "### 4.1 Data Set\n",
    "Run the cell below to generate a data set and split it into training, cross-validation (CV) and test sets. In this example, we're increasing the percentage of cross-validation data points for emphasis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and split data set\n",
    "X, y, centers, classes, std = gen_blobs()\n",
    "\n",
    "# split the data. Large CV population for demonstration\n",
    "X_train, X_, y_train, y_ = train_test_split(X,y,test_size=0.50, random_state=1)\n",
    "X_cv, X_test, y_cv, y_test = train_test_split(X_,y_,test_size=0.20, random_state=1)\n",
    "print(\"X_train.shape:\", X_train.shape, \"X_cv.shape:\", X_cv.shape, \"X_test.shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4.2\"></a>\n",
    "### 4.2 Evaluating categorical model by calculating classification error\n",
    "The evaluation function for categorical models used here is simply the fraction of incorrect predictions:  \n",
    "$$ J_{cv} =\\frac{1}{m}\\sum_{i=0}^{m-1} \n",
    "\\begin{cases}\n",
    "    1, & \\text{if $\\hat{y}^{(i)} \\neq y^{(i)}$}\\\\\n",
    "    0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "<a name=\"ex02\"></a>\n",
    "### Exercise 2\n",
    "\n",
    "Below, complete the routine to calculate classification error. Note, in this lab, target values are the index of the category and are not [one-hot encoded](https://en.wikipedia.org/wiki/One-hot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2\n",
    "# GRADED CELL: eval_cat_err\n",
    "def eval_cat_err(y, yhat):\n",
    "    \"\"\" \n",
    "    Calculate the categorization error\n",
    "    Args:\n",
    "      y    : (ndarray  Shape (m,) or (m,1))  target value of each example\n",
    "      yhat : (ndarray  Shape (m,) or (m,1))  predicted value of each example\n",
    "    Returns:|\n",
    "      cerr: (scalar)             \n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    incorrect = 0\n",
    "    for i in range(m):\n",
    "        if y[i] != yhat[i]:\n",
    "            incorrect += 1\n",
    "    cerr = incorrect/m\n",
    "    \n",
    "    return(cerr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## 5 - Model Complexity\n",
    "Below, you will build two models. A complex model and a simple model. You will evaluate the models to determine if they are likely to overfit or underfit.\n",
    "\n",
    "###  5.1 Complex model\n",
    "\n",
    "<a name=\"ex03\"></a>\n",
    "### Exercise 3\n",
    "Below, compose a three-layer model:\n",
    "* Dense layer with 120 units, relu activation\n",
    "* Dense layer with 40 units, relu activation\n",
    "* Dense layer with 6 units and a linear activation (not softmax)  \n",
    "Compile using\n",
    "* loss with `SparseCategoricalCrossentropy`, remember to use  `from_logits=True`\n",
    "* Adam optimizer with learning rate of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(units=120, activation='relu'),\n",
    "        Dense(units=40, activation='relu'),\n",
    "        Dense(units=6, activation='linear'),\n",
    "    ]\n",
    "\n",
    ")\n",
    "model.compile(optimizer=Adam(learning_rate=0.01), loss=SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a model for plotting routines to call\n",
    "model_predict = lambda Xl: np.argmax(tf.nn.softmax(model.predict(Xl)).numpy(),axis=1)\n",
    "plt_nn(model_predict,X_train,y_train, classes, X_cv, y_cv, suptitle=\"Complex Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"5.1\"></a>\n",
    "### 5.1 Simple model\n",
    "Now, let's try a simple model\n",
    "\n",
    "<a name=\"ex04\"></a>\n",
    "### Exercise 4\n",
    "\n",
    "Below, compose a two-layer model:\n",
    "* Dense layer with 6 units, relu activation\n",
    "* Dense layer with 6 units and a linear activation. \n",
    "Compile using\n",
    "* loss with `SparseCategoricalCrossentropy`, remember to use  `from_logits=True`\n",
    "* Adam optimizer with learning rate of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4\n",
    "# GRADED CELL: model_s\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "model_s = Sequential(\n",
    "    [\n",
    "        ### START CODE HERE ### \n",
    "        Dense(units=6, activation='relu'),\n",
    "        Dense(units=6, activation='linear')\n",
    "        \n",
    "        ### END CODE HERE ### \n",
    "    ], name = \"Simple\"\n",
    ")\n",
    "model_s.compile(\n",
    "    ### START CODE HERE ### \n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=Adam(learning_rate=0.01),\n",
    "    ### START CODE HERE ### \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"6\"></a>\n",
    "## 6 - Regularization\n",
    "As in the case of polynomial regression, one can apply regularization to moderate the impact of a more complex model. Let's try this below.\n",
    "\n",
    "<a name=\"ex05\"></a>\n",
    "### Exercise 5\n",
    "\n",
    "Reconstruct your complex model, but this time include regularization.\n",
    "Below, compose a three-layer model:\n",
    "* Dense layer with 120 units, relu activation, `kernel_regularizer=tf.keras.regularizers.l2(0.1)`\n",
    "* Dense layer with 40 units, relu activation, `kernel_regularizer=tf.keras.regularizers.l2(0.1)`\n",
    "* Dense layer with 6 units and a linear activation. \n",
    "Compile using\n",
    "* loss with `SparseCategoricalCrossentropy`, remember to use  `from_logits=True`\n",
    "* Adam optimizer with learning rate of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C5\n",
    "# GRADED CELL: model_r\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "model_r = Sequential(\n",
    "    [\n",
    "        ### START CODE HERE ### \n",
    "        Dense(units=120, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.1)),\n",
    "        Dense(units=40, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.1)),\n",
    "        Dense(units=6, activation='linear')\n",
    "        \n",
    "        \n",
    "        ### START CODE HERE ### \n",
    "    ], name= None\n",
    ")\n",
    "model_r.compile(\n",
    "    ### START CODE HERE ### \n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=Adam(learning_rate=0.01),\n",
    "    ### START CODE HERE ### \n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
